{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Before you start profiling your code! ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Premature optimization is the root of all evil\" - Donald Knuth\n",
    "\n",
    "- It’s usually **more important that your code runs correctly** according to the business requirements and that other team members can understand it rather than it being the most efficient solution.\n",
    "\n",
    "- The actual **time-saver might be elsewhere**. For example, having the ability to quickly extend your code with new features to meet user needs.\n",
    "\n",
    "- Sometimes, the **return on investment** in performance optimizations just isn’t worth the effort. If you only run your code once or twice, or if it takes longer to improve the code than execute it, then what’s the point?\n",
    "\n",
    "- Code will often become faster just as a result of **fixing the bugs and refactoring**. One of the creators of Erlang once said: \"Make it work, then make it beautiful, then if you really, really have to, make it fast. 90 percent of the time, if you make it beautiful, it will already be fast. So really, just make it beautiful!\" — Joe Armstrong\n",
    "\n",
    "- A performance profiler is a valuable tool for identifying hot spots in existing code, but it won’t tell you how to write efficient code from the start. It’s often the **choice of the underlying algorithm or data structure** that can make the biggest difference to performance. Even when you throw the most advanced hardware available on the market at some computational problem, an algorithm with a poor time or space complexity may never finish in a reasonable time.\n",
    "\n",
    "Therefore, optimize performance as a final step if it's necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software profiling is the process of collecting and analyzing various metrics of a running program to identify performance bottlenecks.\n",
    "\n",
    "These bottlenecks can happen due to a number of reasons: including excessive memory use, inefficient CPU utilization, or a suboptimal data layout, which will result in frequent cache misses that increase latency. (Bottlenecks might lie not in the underlying code’s execution time but in network communication.)\n",
    "\n",
    "Profilers can come with significant overhead, which can slow down code, so this needs to be taken into account, especially when running a profiler as part of a production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of python profilers. You should pick the right tool for the job.\n",
    "- Timers like the `time` and `timeit` standard library modules, or the `codetiming` third-party package\n",
    "- Deterministic profilers like `profile`, `cProfile`, and `line_profiler`\n",
    "- Statistical profilers like `Pyinstrument` and the Linux `perf` profiler\n",
    "- Scalene\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time` for measuring the exact execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time module is versatile and quick to set up, making it suitable for temporary checks. It’ll give you a faithful impression of runtime in real-world conditions, taking into account factors like the current system load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeper()\n",
      " Real time: 1.75 seconds\n",
      " CPU time: 0.00 seconds\n",
      "\n",
      "spinlock()\n",
      " Real time: 1.95 seconds\n",
      " CPU time: 1.95 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ask the OS to suspend the current thread of execution for 1.75 secs\n",
    "# during this time the function remains dormant without occupying the CPU\n",
    "def sleeper():\n",
    "    time.sleep(1.75)\n",
    "\n",
    "# perform busy waiting, wasting CPU cycles without doing any useful work\n",
    "def spinlock():\n",
    "    for _ in range(100_000_000):\n",
    "        pass\n",
    "\n",
    "\n",
    "for function in sleeper, spinlock:\n",
    "    # get wall-clock time and CPU time\n",
    "    t1 = time.perf_counter(), time.process_time()\n",
    "    function()\n",
    "    t2 = time.perf_counter(), time.process_time()\n",
    "    print(f\"{function.__name__}()\")\n",
    "    print(f\" Real time: {t2[0] - t1[0]:.2f} seconds\")\n",
    "    print(f\" CPU time: {t2[1] - t1[1]:.2f} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `timeit` for benchmarking code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `timeit` module accounts from factors such as system load, garbage collection or other processes that might be running concurrently that might skew timing results. The timeit module helps to mitigate these factors, providing a more accurate measure of code execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 ms ± 7.33 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Average time is 0.28 seconds'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "# A recursive function that calculates the nth element of the Fibonacci sequence\n",
    "def fib(n):\n",
    "    return n if n < 2 else fib(n - 2) + fib(n - 1)\n",
    "\n",
    "# repeat 100 times\n",
    "iterations = 100\n",
    "total_time = timeit(\"fib(30)\", number=iterations, globals=globals())\n",
    "\n",
    "# with magic command\n",
    "# %timeit fib(30)\n",
    "\n",
    "# average to average out random fluctuations in execution time that may come from other processes running on your computer.\n",
    "f\"Average time is {total_time / iterations:.2f} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cProfile` for collecting detailed runtime statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cProfile` is a deterministic profiler, which can help you answer questions like how many times a particular function was called or how much total time was spent inside that function. A deterministic profiler can give you reproducible results under the same conditions because it traces all function calls in your program.\n",
    "\n",
    "You can use `cProfile` against your whole program in the command line or profile a narrow code fragment programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fib(35) = 9227465\n",
      "         29860736 function calls (34 primitive calls) in 7.329 seconds\n",
      "\n",
      "   Ordered by: call count\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "29860703/1    7.329    0.000    7.329    7.329 1786731988.py:5(fib)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:444(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:465(_schedule_flush)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:535(write)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:529(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1059(_wait_for_tstate_lock)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1113(is_alive)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:613(send)\n",
      "        1    0.000    0.000    0.000    0.000 cProfile.py:50(create_stats)\n",
      "        1    0.000    0.000    0.000    0.000 pstats.py:107(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 pstats.py:117(init)\n",
      "        1    0.000    0.000    0.000    0.000 pstats.py:136(load_stats)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:90(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:203(schedule)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cProfile import Profile\n",
    "from pstats import SortKey, Stats\n",
    "\n",
    "\n",
    "def fib(n):\n",
    "    return n if n < 2 else fib(n - 2) + fib(n - 1)\n",
    "\n",
    "\n",
    "with Profile() as profile:\n",
    "    print(f\"{fib(35) = }\")\n",
    "    (Stats(profile).strip_dirs().sort_stats(SortKey.CALLS).print_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fib() with a relatively small input value of 35 results in nearly thirty million recursive calls! \n",
    "\n",
    "Most of these recursive calls are redundant because they keep calculating the same values over and over again (because of `fib(n-2) + fib(n-1)`).\n",
    "\n",
    "Memoization can be used as a quick optimization to to cache the intermediate results. That way, you’ll calculate each Fibonacci number once and reuse the cached result for subsequent calls to fib()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pyinstrument` to take snapshots of the call stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To lower the profiler’s overhead, you can use statistical profiling and only collect metrics once in a while by taking a sample of snapshots of the call stack.\n",
    "\n",
    "Statistical profilers don't provide the same level of detail as deterministic ones but they can be adjusted to reduce overhead (deterministic profilers monitor all function calls across your application).\n",
    "\n",
    "The output of pyinstrument shows the frames in the the call stack. See example below.\n",
    "\n",
    "Note: `Pyinstrument` can't handle code that runs in multiple threads or calls functions implemented in C extension modules, such as numpy or pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  _     ._   __/__   _ _  _  _ _/_   Recorded: 11:17:13  Samples:  201\n",
    " /_//_/// /_\\ / //_// / //_'/ //     Duration: 20.150    CPU time: 20.149\n",
    "/   _/                      v4.5.0\n",
    "\n",
    "Program:\n",
    "\n",
    "20.100 <module>  <stdin>:1\n",
    "└─ 20.100 estimate_pi  <stdin>:1\n",
    "      [12 frames hidden]  <stdin>, random, <built-in>\n",
    "         19.200 <genexpr>  <stdin>:2\n",
    "         ├─ 12.900 point  <stdin>:1\n",
    "         │  ├─ 8.000 Random.uniform  random.py:520\n",
    "         │  │  ├─ 6.400 [self]  None\n",
    "         │  └─ 4.900 [self]  None\n",
    "         ├─ 4.800 [self]  None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linux `perf` profiler for counting hardware and system events on linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`perf` can provide detailed information about the entire stack, including hardware events, system calls, library code, and more. Additionally, its overhead is small and adjustable.\n",
    "\n",
    "Note: The perf profiler is only available on Linux, as it’s baked into the operating system’s kernel. However, you’ll usually need to install an extra system package to bring the accompanying command-line utility tool for accessing the underlying kernel subsystem.\n",
    "\n",
    "You’ll also need to be able to build and install Python from source code using special compiler flags for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalene python profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalene is a high-performance CPU, GPU and memory profiler for Python that does a number of things that other Python profilers do not and cannot do. It runs orders of magnitude faster than many other profilers while delivering far more detailed information. It is also the first profiler ever to incorporate AI-powered proposed optimizations.\n",
    "\n",
    "Benefits of scalene:\n",
    "- Small overhead\n",
    "- CPU, GPU and memory profiling\n",
    "- Automatically detects memory leaks\n",
    "- Can separate out python vs native time (in-built C functions)\n",
    "- Can deal with multi-threads\n",
    "\n",
    "[Link to github repo containing more details](https://github.com/plasma-umass/scalene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
